#!/usr/bin/env python2
# vim:fileencoding=utf-8

from __future__ import (
    unicode_literals,
    division,
    absolute_import,
    print_function
)
import json
import datetime
from dateutil import relativedelta
from dateutil.parser import parse as dt_parse
from dateutil.tz import tzutc

from calibre.web.feeds.news import BasicNewsRecipe
from calibre.ebooks.BeautifulSoup import BeautifulSoup as Soup
from calibre.ebooks.BeautifulSoup import Comment as SoupComment


class TheEconomistRecipe(BasicNewsRecipe):
    title = "The Economist"
    __author__ = "Brendon Crawford"
    oldest_article = 13
    max_articles_per_feed = 200
    auto_cleanup = True
    use_embedded_content = False
    publication_type = "magazine"
    needs_subscription = True
    handle_gzip = True
    timefmt = ""

    x_base_url = "https://www.economist.com"

    def get_cover_url(self):
        soup = self.x_get_page_soup()
        imgelm = (
            soup.find(
                "img",
                attrs={
                    "class": "print-edition__cover-widget__image"
                }
            )
        )
        if imgelm is None:
            return None
        if not imgelm.has_key("src"):
            return None
        path = imgelm["src"]
        src = "".join([self.x_base_url, path])
        print("IMAGE: %s" % src)
        return src

    def x_get_page_soup(self):
        content = self.x_fetch_page()
        soup = Soup(content)
        return soup

    def get_browser(self):
        br = super(TheEconomistRecipe, self).get_browser()
        if (self.username is not None) and (self.password is not None):
            login_url = "/".join([self.x_base_url, "user", "login"])
            br.open(login_url)
            br.select_form(nr=1)
            br["name"] = self.username
            br["pass"] = self.password
            br["persistent_login"] = ["1"]
            br.submit()
        return br

    def x_fetch_url(self, url):
        resp = self.browser.open(url)
        data = resp.read()
        return data

    def x_fetch_page(self):
        start_url = "/".join([self.x_base_url, "printedition"])
        content = self.x_fetch_url(start_url)
        return content

    def x_get_soup_content(self, soup):
        def filt(text):
            return (
                (unicode(text.string).strip(u"\u2022\u0020") == u"") or
                isinstance(text, SoupComment)
            )
        comments = soup.findAll(text=filt)
        for comment in comments:
            comment.extract()
        contents = u" ".join(soup.contents).strip()
        return contents

    def x_fetch_articles(self):
        soup = self.x_get_page_soup()
        metapubdate = (
            soup.find(
                "meta",
                attrs={
                    "name": "pubdate"
                }
            )
        )
        if metapubdate is None:
            dt = datetime.datetime.utcnow()
        else:
            dt = dt_parse(metapubdate["content"].strip())
        dt_str = dt.isoformat()
        divmain = (
            soup.find(
                "div",
                attrs={
                    "class": (
                        "main-content__main-column print-edition__content"
                    )
                }
            )
        )
        if divmain is None:
            print("DIVMAIN none")
            return None
        ul = (
            divmain.find(
                "ul",
                attrs={
                    "class": "list"
                }
            )
        )
        if ul is None:
            print("UL none")
            return None
        items = (
            ul.findAll(
                "li",
                attrs={
                    "class": "list__item"
                },
                recursive=False
            )
        )
        if (items is None) or (len(items) == 0):
            print("ITEMS none")
            return None
        feeds = []
        for item in items:
            divtitle = (
                item.find(
                    "div",
                    attrs={
                        "class": "list__title"
                    }
                )
            )
            if divtitle is None:
                print("DIVTITLE none")
                continue
            feed_title = divtitle.string.strip()
            links = (
                item.findAll(
                    "a",
                    attrs={
                        "class": "link-button list__link"
                    },
                    recursive=False
                )
            )
            if (links is None) or (len(links) == 0):
                continue
            articles = []
            for link in links:
                if not link.has_key("href"):
                    continue
                url = "".join([self.x_base_url, link["href"]])
                ##
                ## This is usually the three titles under
                ##
                ## "The world this week"
                ##   - Politics this week
                ##   - Business this week
                ##   - KAL's cartoon
                ##
                spantitlesub = (
                    link.find(
                        "span",
                        attrs={
                            "class": "print-edition__link-title-sub"
                        }
                    )
                )
                ##
                ## This is usually the black subtitle underneath
                ## the red title.
                ##
                spanflytitle = (
                    link.find(
                        "span",
                        attrs={
                            "class": "print-edition__link-flytitle"
                        }
                    )
                )
                ##
                ## This is usually the red main title above the black
                ## subtitle
                ##
                spantitle = (
                    link.find(
                        "span",
                        attrs={
                            "class": "print-edition__link-title"
                        }
                    )
                )
                ##
                ## Build titles
                ## First we have to strip out comments
                ##
                if (
                        (spantitlesub is not None)
                ):
                    title = self.x_get_soup_content(spantitlesub)
                    subtitle = ""
                elif (
                        (spantitle is not None) and
                        (spanflytitle is None)
                ):
                    title = self.x_get_soup_content(spantitle)
                    subtitle = ""
                elif (
                        (spantitle is None) and
                        (spanflytitle is not None)
                ):
                    title = self.x_get_soup_content(spanflytitle)
                    subtitle = ""
                elif (
                        (spantitle is not None) and
                        (spanflytitle is not None)
                ):
                    title = (
                        ": ".join([
                            self.x_get_soup_content(spantitle),
                            self.x_get_soup_content(spanflytitle)
                        ])
                    )
                    subtitle = self.x_get_soup_content(spanflytitle)
                else:
                    title = "TITLE NOT FOUND"
                    subtitle = ""
                ##
                ## Build output
                ##
                article = {
                    "title": title,
                    "url": url,
                    "date": dt_str,
                    "description": subtitle,
                    "content": ""
                }
                articles.append(article)
            feeds.append((
                feed_title,
                articles
            ))
        return feeds

    def parse_index(self):
        res = self.x_fetch_articles()
        if res is None:
            return []
        return res
