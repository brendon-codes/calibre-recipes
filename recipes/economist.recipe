#!/usr/bin/env python2
# vim:fileencoding=utf-8

from __future__ import (
    unicode_literals,
    division,
    absolute_import,
    print_function
)
import json
import datetime
from dateutil import relativedelta
from dateutil.tz import tzutc

from calibre.web.feeds.news import BasicNewsRecipe
from calibre.ebooks.BeautifulSoup import BeautifulSoup as Soup


class TheEconomistRecipe(BasicNewsRecipe):
    title = "The Economist"
    __author__ = "Brendon Crawford"
    oldest_article = 13
    max_articles_per_feed = 200
    auto_cleanup = True
    use_embedded_content = False
    publication_type = "magazine"
    needs_subscription = True
    handle_gzip = True
    timefmt = ""

    x_base_url = "https://www.economist.com"

    def get_cover_url(self):
        soup = self.x_get_page_soup()
        imgelm = (
            soup.find(
                "img",
                attrs={
                    "class": "print-edition__cover-widget__image"
                }
            )
        )
        if imgelm is None:
            return None
        if not imgelm.has_key("src"):
            return None
        path = imgelm["src"]
        src = "".join([self.x_base_url, path])
        print("IMAGE: %s" % src)
        return src

    def x_get_page_soup(self):
        content = self.x_fetch_page()
        soup = Soup(content)
        return soup

    def get_browser(self):
        br = super(TheEconomistRecipe, self).get_browser()
        if (self.username is not None) and (self.password is not None):
            login_url = "/".join([self.x_base_url, "user", "login"])
            br.open(login_url)
            br.select_form(nr=1)
            br["name"] = self.username
            br["pass"] = self.password
            br["persistent_login"] = ["1"]
            br.submit()
        return br

    def x_fetch_url(self, url):
        resp = self.browser.open(url)
        data = resp.read()
        return data

    def x_fetch_page(self):
        start_url = "/".join([self.x_base_url, "printedition"])
        content = self.x_fetch_url(start_url)
        return content

    def x_fetch_articles(self):
        soup = self.x_get_page_soup()
        divmain = (
            soup.find(
                "div",
                attrs={
                    "class": (
                        "main-content__main-column print-edition__content"
                    )
                }
            )
        )
        if divmain is None:
            print("DIVMAIN none")
            return None
        ul = (
            divmain.find(
                "ul",
                attrs={
                    "class": "list"
                }
            )
        )
        if ul is None:
            print("UL none")
            return None
        items = (
            ul.findAll(
                "li",
                attrs={
                    "class": "list__item"
                },
                recursive=False
            )
        )
        if (items is None) or (len(items) == 0):
            print("ITEMS none")
            return None
        feeds = []
        for item in items:
            divtitle = (
                item.find(
                    "div",
                    attrs={
                        "class": "list__title"
                    }
                )
            )
            if divtitle is None:
                print("DIVTITLE none")
                continue
            feed_title = divtitle.string.strip()
            links = (
                item.findAll(
                    "a",
                    attrs={
                        "class": "link-button list__link"
                    },
                    recursive=False
                )
            )
            if (links is None) or (len(links) == 0):
                continue
            articles = []
            for link in links:
                if not link.has_key("href"):
                    continue
                url = "".join([self.x_base_url, link["href"]])
                spanflytitle = (
                    link.find(
                        "span",
                        attrs={
                            "class": "print-edition__link-flytitle"
                        }
                    )
                )
                spantitle = (
                    link.find(
                        "span",
                        attrs={
                            "class": "print-edition__link-title"
                        }
                    )
                )
                if (spanflytitle is None):
                    title = (
                        link.string.strip() if
                        (link.string is not None) else
                        "TITLE NOT FOUND"
                    )
                else:
                    title = spanflytitle.string.strip()
                if (spantitle is None):
                    subtitle = ""
                else:
                    subtitle = spantitle.string.strip()
                article = {
                    "title": title,
                    "url": url,
                    "date": 1,
                    "description": subtitle,
                    "content": ""
                }
                articles.append(article)
            feeds.append((
                feed_title,
                articles
            ))
        return feeds

    def parse_index(self):
        res = self.x_fetch_articles()
        if res is None:
            return []
        return res
