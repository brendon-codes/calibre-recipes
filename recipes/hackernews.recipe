#!/usr/bin/env python2
# vim:fileencoding=utf-8

from __future__ import (
    unicode_literals,
    division,
    absolute_import,
    print_function
)
import json
import datetime
from urlparse import urlparse

from calibre.ebooks.BeautifulSoup import BeautifulSoup as Soup
from calibre.ptempfile import PersistentTemporaryFile
from calibre.web.feeds.news import BasicNewsRecipe


class HackerNewsRecipe(BasicNewsRecipe):
    title = "Hacker News"
    __author__ = "Brendon Crawford"
    oldest_article = 2
    max_articles_per_feed = 100
    auto_cleanup = True
    use_embedded_content = False
    publication_type = "blog"
    timefmt = ""
    temp_files = []
    auto_cleanup_keep = (
        "//article[@id='hn-comments'] | //article[@id='hn-comments']::*"
    )

    FEED_TITLE = "All"
    ARTICLE_COUNT = 25
    ARTICLE_SCORE = 70
    ARTICLE_TYPES = frozenset([
        "story"
    ])
    ARTICLE_DOMAINS_EXCLUDE = [
        "nytimes.com",
        "bloomberg.com",
        "economist.com",
        "theatlantic.com",
        "techcrunch.com"
    ]
    COMMENTS_COUNT = 20
    COMMENT_SCORE = 2
    ARTICLE_DATE_HOURS = 24

    def x_fetch_url(self, url):
        br = self.get_browser()
        resp = br.open(url)
        data = resp.read()
        return data

    def x_fetch_url_json(self, url):
        content = self.x_fetch_url(url)
        if content is None:
            return None
        obj = json.loads(content)
        return obj

    def x_get_article(self, article_id):
        base = "https://hacker-news.firebaseio.com/v0/item/{id}.json"
        url = base.format(id=article_id)
        article = self.x_fetch_url_json(url)
        return article

    def x_should_skip_art_score(self, article):
        return (article["score"] < self.ARTICLE_SCORE)

    def x_should_skip_art_type(self, article):
        return (article["type"] not in self.ARTICLE_TYPES)

    def x_should_skip_art_domain(self, article):
        if "url" not in article:
            return True
        url = article["url"]
        parts = urlparse(url)
        domain = parts.netloc
        for exc_domain in self.ARTICLE_DOMAINS_EXCLUDE:
            exc_dotdomain = "".join([".", exc_domain])
            if (
                    (exc_domain in domain) or
                    (exc_dotdomain in domain)
            ):
                return True
        return False

    def x_should_skip_article(self, article):
        if self.x_should_skip_art_score(article):
            return True
        if self.x_should_skip_art_type(article):
            return True
        if self.x_should_skip_art_domain(article):
            return True
        return False

    def x_fetch_api_comments(self, comments_map, pre_comment_ids):
        base = "https://hacker-news.firebaseio.com/v0/item/{id}.json"
        comments = []
        pre_comments = []
        comment_ids = []
        outer = 0
        for pre_comment_id in pre_comment_ids:
            if pre_comment_id not in comments_map:
                continue
            pre_comments.append({
                "id": pre_comment_id,
                "score": comments_map[pre_comment_id]
            })
        outer_comments = self.x_filter_comments(pre_comments)
        for outer_comment in outer_comments:
            if outer > 20:
                break
            comment_id = outer_comment["id"]
            if comment_id not in comments_map:
                continue
            url = base.format(id=comment_id)
            comment_json = self.x_fetch_url_json(url)
            if comment_json is None:
                continue
            comments.append({
                "id": comment_id,
                "score": comments_map[comment_id],
                "date": (
                    datetime.datetime.utcfromtimestamp(
                        int(comment_json["time"])
                    )
                ),
                "depth": 1,
                "text": comment_json["text"]
            })
            outer += 1
            inner = 0
            for subcomment_id in comment_json["kids"]:
                if inner > 2:
                    break
                if subcomment_id not in comments_map:
                    continue
                suburl = base.format(id=subcomment_id)
                subcomment_json = self.x_fetch_url_json(suburl)
                comments.append({
                    "id": subcomment_id,
                    "score": comments_map[subcomment_id],
                    "date": (
                        datetime.datetime.utcfromtimestamp(
                            int(subcomment_json["time"])
                        )
                    ),
                    "depth": 2,
                    "text": subcomment_json["text"]
                })
                inner += 1
        return comments


    def x_get_comments_map(self, article_id):
        base = "https://news.ycombinator.com/item?id={id}"
        url = base.format(id=article_id)
        content = self.x_fetch_url(url)
        soup = Soup(content)
        com_rows = (
            soup.findAll(
                "tr",
                attrs={
                    "class": "athing comtr "
                },
                recursive=True
            )
        )
        comments = {}
        for com_row in com_rows:
            comment = self.x_parse_comment(com_row)
            if comment is None:
                continue
            comments[comment["id"]] = comment["score"]
        return comments

    def x_parse_comment(self, com_row):
        if not com_row.has_key("id"):
            return None
        atogg = (
            com_row.find(
                "a",
                attrs={
                    "class": "togg"
                }
            )
        )
        if atogg is None:
            return None
        if not atogg.has_key("n"):
            return None
        ##
        ## Build output
        ##
        comment = {}
        comment["id"] = int(com_row["id"])
        comment["score"] = int(atogg["n"])
        return comment

    def x_filter_comments(self, all_comments):
        sorted_coms = (
            sorted(
                all_comments,
                key=lambda c: c["score"],
                reverse=True
            )
        )
        filt_comments = (
            filter(lambda c: c["score"] >= self.COMMENT_SCORE, sorted_coms)
        )
        comments = filt_comments[:self.COMMENTS_COUNT]
        return comments

    def x_process_comments(self, article):
        comments_map = self.x_get_comments_map(article["id"])
        comments = (
            self.x_fetch_api_comments(comments_map, article["comment_ids"])
        )
        html = self.x_format_comments(article, comments)
        return html

    def x_format_comments(self, article, comments):
        page_title = "Comments: {title}".format(title=article["title"])
        out = "<!doctype html>\n"
        out += "<html lang='en'>\n"
        out += "<head>\n"
        out += "<meta charset='utf-8' />\n"
        out += (
            "<title>{title}</title>\n".format(title=page_title)
        )
        out += "</head>\n"
        out += "<body>\n"
        out += "<article id='hn-comments'>\n"
        out += (
            "<h1>{title}</h1>\n".format(title=page_title)
        )
        out += "<div>\n\n"
        for comment in comments:
            stars = " ".join(map(lambda x: "*", xrange(comment["depth"])))
            out += "<div>\n"
            out += (
                "<h3>{stars} | {score} | {date}</h3>\n".format(
                    stars=stars,
                    score=comment["score"],
                    date=comment["date"].strftime('%Y-%m-%d %H:%M:%S')
                )
            )
            out += "<div>\n"
            out += comment["text"]
            out += "\n"
            out += "</div>\n"
            out += "</div>\n\n"
        out += "</div>\n"
        out += "</article>\n"
        out += "</body>\n"
        out += "</html>\n"
        return out

    def x_fetch_articles(self):
        url = "https://hacker-news.firebaseio.com/v0/topstories.json"
        new_article_ids = self.x_fetch_url_json(url)
        if new_article_ids is None:
            return []
        i = 0
        out_articles = []
        now = datetime.datetime.utcnow()
        for article_id in new_article_ids:
            print("FETCHING ARTICLE: %s" % article_id)
            article = self.x_get_article(article_id)
            if article is None:
                continue
            if self.x_should_skip_article(article):
                continue
            article_date = (
                datetime.datetime.utcfromtimestamp(
                    int(article["time"])
                )
            )
            offset = now - article_date
            cutoff = (self.ARTICLE_DATE_HOURS * 3600)
            offsecs = int(offset.total_seconds())
            if offsecs > cutoff:
                continue
            out_articles.append({
                "id": article["id"],
                "title": article["title"],
                "url": article["url"],
                "score": article["score"],
                "date": article_date,
                "comment_ids": article["kids"]
            })
            i += 1
        final_articles = (
            sorted(
                out_articles,
                key=lambda a: a["score"],
                reverse=True
            )
        )
        limit_articles = final_articles[:self.ARTICLE_COUNT]
        return limit_articles

    def x_get_articles(self):
        articles = self.x_fetch_articles()
        out_articles = []
        for article in articles:
            print("PROCESSING ARTICLE: %s" % article["id"])
            format_date = article["date"].strftime('%Y-%m-%d %H:%M:%S')
            article_title = (
                "[{score}] {title}".format(
                    score=article["score"],
                    title=article["title"]
                )
            )
            comments_title = "[Comments] {title}".format(title=article["title"])
            comments_html = self.x_process_comments(article)
            out_articles.append({
                "title": article_title,
                "url": article["url"],
                "date": format_date,
                "comments_title": comments_title,
                "comments_html": comments_html
            })
        #print(out_articles)
        return out_articles

    def x_save_comment_url(self, article):
        tmp = PersistentTemporaryFile('_fa.html')
        tmp.write(article["comments_html"])
        tmp.close()
        self.temp_files.append(tmp)
        url = "".join(["file://", tmp.name])
        return url

    def parse_index(self):
        items = self.x_get_articles()
        articles = []
        for item in items:
            articles.append({
                "title": item["title"],
                "url": item["url"],
                "date": item["date"],
                "description": item["title"],
                "content": ""
            })
            articles.append({
                "title": item["comments_title"],
                "url": self.x_save_comment_url(item),
                "date": item["date"],
                "description": item["comments_title"],
                "content": ""
            })
        feeds = []
        feeds.append((
            self.FEED_TITLE,
            articles
        ))
        return feeds
