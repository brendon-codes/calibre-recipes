#!/usr/bin/env python2
# vim:fileencoding=utf-8

u"""
Hacker News Recipe

Sometimes with this recipe, you might get a SplitError.
If you are using Kindle, you can usually solve this with the
following steps:

1. Select "Preferences"
2. Select "Output Options"
3. Select "EPUB Output"
4. Find "Split files larger than:"
   1. Enter a large value such as "100000 KB".
      For reasons unknown, as of Calibre v3.6, entering
      a value of "0" does not seem to work as expected.
5. Click "Apply"
6. Click "Close"

Keep in mind that splitting with a value over 240KB could
cause various problems and errors for some devices, although
I have not observed any problems for Kindle.

"""

from __future__ import (
    unicode_literals,
    division,
    absolute_import,
    print_function
)
import time
import sys
import json
import datetime
from pprint import pprint
from dateutil import tz
from urlparse import urlparse
from urllib2 import HTTPError, URLError

from calibre.ebooks.BeautifulSoup import BeautifulSoup as Soup
from calibre.ptempfile import PersistentTemporaryFile
from calibre.web.feeds.news import BasicNewsRecipe


class HackerNewsRecipe(BasicNewsRecipe):
    title = u"Hacker News"
    __author__ = u"Brendon Crawford"
    oldest_article = 2
    max_articles_per_feed = 100
    auto_cleanup = True
    use_embedded_content = False
    publication_type = u"blog"
    timefmt = u""
    temp_files = []
    language = u"eng"
    encoding = u"utf8"
    recursions = 0
    no_stylesheets = True
    remove_javascript = True
    remove_tags = [
        {u"name": u"style"},
        {u"name": u"img"},
        {u"name": u"picture"},
        {u"name": u"source"},
        {u"name": u"video"},
        {u"name": u"link", u"attrs": {u"rel": u"stylesheet"}}
    ]
    remove_attributes = [u"style", u"href"]
    auto_cleanup_keep = (
        u"//article[@id='hn-comments'] | //article[@id='hn-comments']::*"
    )

    DEBUG_ARTICLE_FETCH = False
    ARTICLE_COUNT = 20
    ARTICLE_TYPES = frozenset([
        u"story"
    ])
    ARTICLE_DOMAINS_EXCLUDE = [
        # u"nytimes.com",
        # u"bloomberg.com",
        # u"economist.com",
        # u"theatlantic.com",
        # u"techcrunch.com"
    ]
    ITEM_BASE_URL = u"https://hacker-news.firebaseio.com/v0/item/{id}.json"
    TOP_BASE_URL = u"https://hacker-news.firebaseio.com/v0/topstories.json"
    COMMENT_COUNTS = [12, 6, 1]
    ARTICLE_DATE_HOURS = [12, 24]

    def preprocess_image(self, img_data, image_url):
        u"""
        Perform some processing on downloaded image data.
        This is called on the raw data before any resizing is done.
        Must return the processed raw data. Return
        None to skip the image.
        u"""
        return None

    def x_fetch_url(self, url):
        br = self.get_browser()
        try:
            resp = br.open(url)
        except (HTTPError, URLError) as e:
            return None
        data = resp.read()
        return data

    def x_fetch_url_json(self, url):
        content = self.x_fetch_url(url)
        if content is None:
            return None
        obj = json.loads(content)
        return obj

    def x_get_article(self, article_id):
        url = self.ITEM_BASE_URL.format(id=article_id)
        article = self.x_fetch_url_json(url)
        if article is None:
            return None
        return article

    def x_should_skip_art_type(self, article):
        return (article[u"type"] not in self.ARTICLE_TYPES)

    def x_should_skip_art_domain(self, article):
        if u"url" not in article:
            return True
        url = article[u"url"]
        domain = urlparse(url).netloc
        for exc_domain in self.ARTICLE_DOMAINS_EXCLUDE:
            exc_dotdomain = u"".join([u".", exc_domain])
            if (
                    (exc_domain in domain) or
                    (exc_dotdomain in domain)
            ):
                return True
        return False

    def x_should_skip_article(self, article):
        if self.x_should_skip_art_type(article):
            return True
        if self.x_should_skip_art_domain(article):
            return True
        return False

    def x_build_comments(
            self,
            comment_ids,
            ret_comments,
            secname,
            depth=0
    ):
        if (depth >= len(self.COMMENT_COUNTS)):
            return True
        if ((comment_ids is None) or (len(comment_ids) == 0)):
            return True
        comment_count = self.COMMENT_COUNTS[depth]
        filt_comments = (
            self.x_filter_comments(
                comment_ids,
                comment_count,
                secname
            )
        )
        for filt_comment in filt_comments:
            comment_id = filt_comment
            url = self.ITEM_BASE_URL.format(id=comment_id)
            comment_json = self.x_fetch_url_json(url)
            if comment_json is None:
                continue
            if (
                    (u"deleted" in comment_json) and
                    (comment_json[u"deleted"] is True)
            ):
                continue
            ret_comments.append({
                u"id": comment_id,
                u"date": (
                    (
                        datetime.datetime.utcfromtimestamp(
                            int(comment_json[u"time"])
                        )
                        .replace(tzinfo=tz.tzutc())
                    ) if
                    (
                        (comment_json is not None) and
                        (u"time" in comment_json)
                    ) else
                    u""
                ),
                u"depth": depth,
                u"text": unicode(comment_json[u"text"])
            })
            sub_comment_ids = (
                comment_json[u"kids"] if
                (u"kids" in comment_json) else
                []
            )
            self.x_build_comments(
                sub_comment_ids,
                ret_comments,
                secname,
                (depth + 1)
            )
        return True

    def x_fetch_api_comments(self, comment_ids, secname):
        comments = []
        self.x_build_comments(comment_ids, comments, secname)
        return comments

    def x_filter_comments(self, comment_ids, count, secname):
        comments = comment_ids[:count]
        return comments

    def x_process_comments(self, article, secname):
        comments = (
            self.x_fetch_api_comments(
                article[u"comment_ids"][:self.COMMENT_COUNTS[0]],
                secname
            )
        )
        comments_count = len(article[u"comment_ids"])
        html = self.x_format_comments(article, comments)
        return {
            u"html": html,
            u"count": comments_count
        }

    def x_format_comments(self, article, comments):
        out = []
        page_title = u"Comments: {title}".format(title=article[u"title"])
        out.append(u"<!doctype html>\n")
        out.append(u"<html lang='en'>\n")
        out.append(u"<head>\n")
        out.append(u"<meta charset='utf-8' />\n")
        out.append(
            u"<title>{title}</title>\n".format(title=page_title)
        )
        out.append(u"</head>\n")
        out.append(u"<body>\n")
        out.append(u"<article id='hn-comments'>\n")
        out.append(
            u"<h1>{title}</h1>\n".format(title=page_title)
        )
        out.append(u"<div>\n\n")
        for comment in comments:
            real_depth = comment[u"depth"] + 1
            stars = u" u".join(map(lambda x: u"+", xrange(real_depth)))
            comment_date = (
                comment[u"date"]
                .astimezone(tz.tzlocal())
                .strftime(u"%Y-%m-%d %I:%M:%S%p")
            )
            out.append(u"<div>\n")
            out.append(
                u"<h3>{stars} | {date}</h3>\n".format(
                    stars=stars,
                    date=comment_date
                )
            )
            out.append(u"<div>\n")
            out.append(comment[u"text"])
            out.append(u"\n")
            out.append(u"</div>\n")
            out.append(u"</div>\n\n")
        out.append(u"</div>\n")
        out.append(u"</article>\n")
        out.append(u"</body>\n")
        out.append(u"</html>\n")
        ret = u"".join(out)
        return ret

    def x_fetch_articles(self):
        url = self.TOP_BASE_URL
        new_article_ids = self.x_fetch_url_json(url)
        if new_article_ids is None:
            return []
        i = 0
        out_articles = []
        now = datetime.datetime.utcnow().replace(tzinfo=tz.tzutc())
        for article_id in new_article_ids:
            self.report_progress(0, u"Fetching Article: %s" % article_id)
            article = self.x_get_article(article_id)
            if article is None:
                continue
            if self.x_should_skip_article(article):
                continue
            article_date = (
                datetime.datetime.utcfromtimestamp(
                    int(article[u"time"])
                )
                .replace(tzinfo=tz.tzutc())
            )
            offset = now - article_date
            cutoff = (max(self.ARTICLE_DATE_HOURS) * 3600)
            offsecs = int(offset.total_seconds())
            if offsecs > cutoff:
                continue
            out_articles.append({
                u"id": article[u"id"],
                u"title": unicode(article[u"title"]),
                u"url": unicode(article[u"url"]),
                u"domain": urlparse(unicode(article[u"url"])).netloc,
                u"score": article[u"score"],
                u"date": article_date,
                u"comment_ids": (
                    article[u"kids"] if
                    (u"kids" in article) else
                    []
                )
            })
            ##
            ## Debug
            ##
            if self.DEBUG_ARTICLE_FETCH and (i > 7):
                break
            i += 1
        return out_articles

    def x_filter_articles(self, articles):
        now = datetime.datetime.utcnow().replace(tzinfo=tz.tzutc())
        def filt_date(article, hour):
            article_date = article[u"date"]
            offset = now - article_date
            cutoff = (hour * 3600)
            offsecs = int(offset.total_seconds())
            ret = offsecs <= cutoff
            return ret
        arts_top = (
            sorted(
                articles,
                key=lambda a: a[u"score"],
                reverse=True
            )
        )
        arts_top_lim_map = {}
        arts_top_lim = {}
        for hour in self.ARTICLE_DATE_HOURS:
            arts_top_lim[hour] = []
            for art in arts_top:
                if not filt_date(art, hour):
                    continue
                if art[u"id"] not in arts_top_lim_map:
                    arts_top_lim_map[art[u"id"]] = art
                    arts_top_lim[hour].append(art)
                if len(arts_top_lim[hour]) >= self.ARTICLE_COUNT:
                    break
        arts_new = (
            filter((lambda a: (a[u"id"] not in arts_top_lim_map)), articles)
        )
        arts_new_lim = list(arts_new[:self.ARTICLE_COUNT])
        ret = {
            u"top": arts_top_lim,
            u"new": arts_new_lim
        }
        return ret

    def x_build_art_details(self, articles, secname):
        out_articles = {}
        for article in articles:
            self.report_progress(0, u"Processing Article: %s" % article[u"id"])
            format_date = article[u"date"].strftime('%Y-%m-%d %H:%M:%S')
            article_title = (
                u"S | {score} | {domain} | {title}".format(
                    score=article[u"score"],
                    title=article[u"title"],
                    domain=article[u"domain"]
                )
            )
            comments_res = self.x_process_comments(article, secname)
            comments_html = comments_res[u"html"]
            comments_count = comments_res[u"count"]
            comments_title = (
                u"C | {count} | {domain} | {title}".format(
                    count=comments_count,
                    title=article[u"title"],
                    domain=article[u"domain"]
                )
            )
            out_articles[article[u"id"]] = {
                u"title": article_title,
                u"url": article[u"url"],
                u"date": format_date,
                u"comments_title": comments_title,
                u"comments_html": comments_html
            }
        return out_articles

    def x_collate_articles(self, articles, dets_map):
        return (
            map(
                (lambda a: dets_map[a[u"id"]]),
                articles
            )
        )

    def x_build_art_details_all(self, filt_arts):
        dets_map = {}
        for hour, filt_arts_top in filt_arts[u"top"].items():
            out_top = self.x_build_art_details(filt_arts_top, u"top")
            dets_map.update(out_top)
        dets_map_new = self.x_build_art_details(filt_arts[u"new"], u"new")
        dets_map.update(dets_map_new)
        return dets_map

    def x_get_articles(self):
        raw_arts = self.x_fetch_articles()
        filt_arts = self.x_filter_articles(raw_arts)
        dets_map = self.x_build_art_details_all(filt_arts)
        ret = {}
        out_new = self.x_collate_articles(filt_arts[u"new"], dets_map)
        ret[u"New"] = out_new
        for hour, filt_arts_top in filt_arts[u"top"].items():
            out_top = self.x_collate_articles(filt_arts_top, dets_map)
            out_top_name = self.x_build_hour_name(u"Top", hour)
            ret[out_top_name] = out_top
        return ret

    def x_build_hour_name(self, sectitle, hour):
        return u"%s %sh" % (sectitle, hour)

    def x_save_comment_url(self, article):
        tmp = PersistentTemporaryFile(u"_fa.html")
        tmp.write(article[u"comments_html"])
        tmp.close()
        self.temp_files.append(tmp)
        url = u"".join([u"file://", tmp.name])
        return url

    def parse_index(self):
        item_grps = self.x_get_articles()
        item_grps_pairs = item_grps.items()
        item_grps_pairs_s = sorted(item_grps_pairs, key=lambda r: r[0])
        feeds = []
        for section, items in item_grps_pairs_s:
            articles = []
            for item in items:
                articles.append({
                    u"title": item[u"title"],
                    u"url": item[u"url"],
                    u"date": item[u"date"],
                    u"description": item[u"title"],
                    u"content": u""
                })
                articles.append({
                    u"title": item[u"comments_title"],
                    u"url": self.x_save_comment_url(item),
                    u"date": item[u"date"],
                    u"description": item[u"comments_title"],
                    u"content": u""
                })
            feeds.append((
                section,
                articles
            ))
        return feeds
