#!/usr/bin/env python2
# vim:fileencoding=utf-8

"""
Hacker News Recipe

There are a couple Calibre python errors you might see when
using this recipe.  These should mostly be fixed by the
patches included in this repo.

Two known bugs:

- cssutils: ValueError
- calibre: SplitError

If you do get these, apply the patches which are included
in the `patches` directory of this repo.
"""

from __future__ import (
    unicode_literals,
    division,
    absolute_import,
    print_function
)
import sys
import json
import datetime
from pprint import pprint
from dateutil import tz
from urlparse import urlparse
from urllib2 import HTTPError, URLError

from calibre.ebooks.BeautifulSoup import BeautifulSoup as Soup
from calibre.ptempfile import PersistentTemporaryFile
from calibre.web.feeds.news import BasicNewsRecipe


class HackerNewsRecipe(BasicNewsRecipe):
    title = "Hacker News"
    __author__ = "Brendon Crawford"
    oldest_article = 2
    max_articles_per_feed = 100
    auto_cleanup = True
    use_embedded_content = False
    publication_type = "blog"
    timefmt = ""
    temp_files = []
    language = "eng"
    encoding = "utf8"
    recursions = 0
    no_stylesheets = True
    remove_javascript = True
    remove_tags = [
        {"name": "style"},
        {"name": "img"},
        {"name": "picture"},
        {"name": "source"},
        {"name": "video"},
        {"name": "link", "attrs": {"rel": "stylesheet"}}
    ]
    remove_attributes = ["style", "href"]
    auto_cleanup_keep = (
        "//article[@id='hn-comments'] | //article[@id='hn-comments']::*"
    )

    DEBUG_ARTICLE_FETCH = False
    ARTICLE_COUNT = 16
    ARTICLE_TYPES = frozenset([
        "story"
    ])
    ARTICLE_DOMAINS_EXCLUDE = [
        # "nytimes.com",
        # "bloomberg.com",
        # "economist.com",
        # "theatlantic.com",
        # "techcrunch.com"
    ]
    ITEM_BASE_URL = "https://hacker-news.firebaseio.com/v0/item/{id}.json"
    COMMENT_COUNTS = [12, 6, 1]
    COMMENT_SCORE = {
        "top": 3,
        "new": 0
    }
    ARTICLE_DATE_HOURS = [12, 24]

    def preprocess_image(self, img_data, image_url):
        """
        Perform some processing on downloaded image data.
        This is called on the raw data before any resizing is done.
        Must return the processed raw data. Return
        None to skip the image.
        """
        return None

    def x_fetch_url(self, url):
        br = self.get_browser()
        try:
            resp = br.open(url)
        except (HTTPError, URLError) as e:
            return None
        data = resp.read()
        return data

    def x_fetch_url_json(self, url):
        content = self.x_fetch_url(url)
        if content is None:
            return None
        obj = json.loads(content)
        return obj

    def x_get_article(self, article_id):
        url = self.ITEM_BASE_URL.format(id=article_id)
        article = self.x_fetch_url_json(url)
        if article is None:
            return None
        return article

    def x_should_skip_art_type(self, article):
        return (article["type"] not in self.ARTICLE_TYPES)

    def x_should_skip_art_domain(self, article):
        if "url" not in article:
            return True
        url = article["url"]
        domain = urlparse(url).netloc
        for exc_domain in self.ARTICLE_DOMAINS_EXCLUDE:
            exc_dotdomain = "".join([".", exc_domain])
            if (
                    (exc_domain in domain) or
                    (exc_dotdomain in domain)
            ):
                return True
        return False

    def x_should_skip_article(self, article):
        if self.x_should_skip_art_type(article):
            return True
        if self.x_should_skip_art_domain(article):
            return True
        return False

    def x_build_comments(
            self,
            comments_map,
            comment_ids,
            ret_comments,
            secname,
            depth=0
    ):
        if (depth >= len(self.COMMENT_COUNTS)):
            return True
        if ((comment_ids is None) or (len(comment_ids) == 0)):
            return True
        comment_count = self.COMMENT_COUNTS[depth]
        #print("COUNT: %d" % comment_count)
        filt_comments = (
            self.x_filter_comments(
                comments_map,
                comment_ids,
                comment_count,
                secname
            )
        )
        for filt_comment in filt_comments:
            comment_id = filt_comment["id"]
            if comment_id not in comments_map:
                continue
            url = self.ITEM_BASE_URL.format(id=comment_id)
            comment_json = self.x_fetch_url_json(url)
            if ("deleted" in comment_json) and (comment_json["deleted"] is True):
                continue
            ret_comments.append({
                "id": comment_id,
                "score": comments_map[comment_id],
                "date": (
                    (
                        datetime.datetime.utcfromtimestamp(
                            int(comment_json["time"])
                        )
                        .replace(tzinfo=tz.tzutc())
                    ) if
                    (
                        (comment_json is not None) and
                        ("time" in comment_json)
                    ) else
                    ""
                ),
                "depth": depth,
                "text": comment_json["text"]
            })
            sub_comment_ids = (
                comment_json["kids"] if
                ("kids" in comment_json) else
                []
            )
            self.x_build_comments(
                comments_map,
                sub_comment_ids,
                ret_comments,
                secname,
                (depth + 1)
            )
        return True

    def x_fetch_api_comments(self, comments_map, comment_ids, secname):
        comments = []
        self.x_build_comments(comments_map, comment_ids, comments, secname)
        return comments

    def x_get_comments_map(self, article_id):
        base = "https://news.ycombinator.com/item?id={id}"
        url = base.format(id=article_id)
        content = self.x_fetch_url(url)
        soup = Soup(content)
        com_rows = (
            soup.findAll(
                "tr",
                attrs={
                    "class": "athing comtr "
                },
                recursive=True
            )
        )
        comments = {}
        for com_row in com_rows:
            comment = self.x_parse_comment(com_row)
            if comment is None:
                continue
            comments[comment["id"]] = comment["score"]
        return comments

    def x_parse_comment(self, com_row):
        if not com_row.has_key("id"):
            return None
        atogg = (
            com_row.find(
                "a",
                attrs={
                    "class": "togg"
                }
            )
        )
        if atogg is None:
            return None
        if not atogg.has_key("n"):
            return None
        ##
        ## Build output
        ##
        comment = {}
        comment["id"] = int(com_row["id"])
        comment["score"] = int(atogg["n"])
        return comment

    def x_filter_comments(self, comments_map, comment_ids, count, secname):
        use_comment_score = self.COMMENT_SCORE[secname]
        out = []
        for pre_comment_id in comment_ids:
            if pre_comment_id not in comments_map:
                continue
            out.append({
                "id": pre_comment_id,
                "score": comments_map[pre_comment_id]
            })
        sorted_coms = (
            sorted(
                out,
                key=lambda c: c["score"],
                reverse=True
            )
        )
        filt_comments = (
            filter(
                (lambda c: c["score"] >= use_comment_score),
                sorted_coms
            )
        )
        comments = filt_comments[:count]
        return comments

    def x_process_comments(self, article, secname):
        comments_map = self.x_get_comments_map(article["id"])
        comments_count = len(comments_map)
        comments = (
            self.x_fetch_api_comments(
                comments_map,
                article["comment_ids"],
                secname
            )
        )
        html = self.x_format_comments(article, comments)
        return {
            "html": html,
            "count": comments_count
        }

    def x_format_comments(self, article, comments):
        page_title = "Comments: {title}".format(title=article["title"])
        out = "<!doctype html>\n"
        out += "<html lang='en'>\n"
        out += "<head>\n"
        out += "<meta charset='utf-8' />\n"
        out += (
            "<title>{title}</title>\n".format(title=page_title)
        )
        out += "</head>\n"
        out += "<body>\n"
        out += "<article id='hn-comments'>\n"
        out += (
            "<h1>{title}</h1>\n".format(title=page_title)
        )
        out += "<div>\n\n"
        for comment in comments:
            real_depth = comment["depth"] + 1
            stars = " ".join(map(lambda x: "+", xrange(real_depth)))
            comment_date = (
                comment["date"]
                .astimezone(tz.tzlocal())
                .strftime('%Y-%m-%d %I:%M:%S%p')
            )
            out += "<div>\n"
            out += (
                "<h3>{stars} | {score} | {date}</h3>\n".format(
                    stars=stars,
                    score=comment["score"],
                    date=comment_date
                )
            )
            out += "<div>\n"
            out += comment["text"]
            out += "\n"
            out += "</div>\n"
            out += "</div>\n\n"
        out += "</div>\n"
        out += "</article>\n"
        out += "</body>\n"
        out += "</html>\n"
        return out

    def x_fetch_articles(self):
        url = "https://hacker-news.firebaseio.com/v0/topstories.json"
        new_article_ids = self.x_fetch_url_json(url)
        if new_article_ids is None:
            return []
        i = 0
        out_articles = []
        now = datetime.datetime.utcnow().replace(tzinfo=tz.tzutc())
        for article_id in new_article_ids:
            self.report_progress(0, "Fetching Article: %s" % article_id)
            article = self.x_get_article(article_id)
            if article is None:
                continue
            if self.x_should_skip_article(article):
                continue
            article_date = (
                datetime.datetime.utcfromtimestamp(
                    int(article["time"])
                )
                .replace(tzinfo=tz.tzutc())
            )
            offset = now - article_date
            cutoff = (max(self.ARTICLE_DATE_HOURS) * 3600)
            offsecs = int(offset.total_seconds())
            if offsecs > cutoff:
                continue
            out_articles.append({
                "id": article["id"],
                "title": article["title"],
                "url": article["url"],
                "domain": urlparse(article["url"]).netloc,
                "score": article["score"],
                "date": article_date,
                "comment_ids": (
                    article["kids"] if
                    ("kids" in article) else
                    []
                )
            })
            ##
            ## Debug
            ##
            if self.DEBUG_ARTICLE_FETCH and (i > 3):
                break
            i += 1
        return out_articles

    def x_filter_articles(self, articles):
        now = datetime.datetime.utcnow().replace(tzinfo=tz.tzutc())
        def filt_date(article, hour):
            article_date = article["date"]
            offset = now - article_date
            cutoff = (hour * 3600)
            offsecs = int(offset.total_seconds())
            ret = offsecs <= cutoff
            return ret
        arts_top = (
            sorted(
                articles,
                key=lambda a: a["score"],
                reverse=True
            )
        )
        arts_top_lim_map = {}
        arts_top_lim = {}
        for hour in self.ARTICLE_DATE_HOURS:
            arts_top_lim[hour] = []
            for art in arts_top:
                if not filt_date(art, hour):
                    continue
                if art["id"] not in arts_top_lim_map:
                    arts_top_lim_map[art["id"]] = art
                    arts_top_lim[hour].append(art)
                if len(arts_top_lim[hour]) >= self.ARTICLE_COUNT:
                    break
        arts_new = (
            filter((lambda a: (a["id"] not in arts_top_lim_map)), articles)
        )
        arts_new_lim = list(arts_new[:self.ARTICLE_COUNT])
        ret = {
            "top": arts_top_lim,
            "new": arts_new_lim
        }
        return ret

    def x_build_art_details(self, articles, secname):
        out_articles = {}
        for article in articles:
            self.report_progress(0, "Processing Article: %s" % article["id"])
            format_date = article["date"].strftime('%Y-%m-%d %H:%M:%S')
            article_title = (
                "S | {score} | {domain} | {title}".format(
                    score=article["score"],
                    title=article["title"],
                    domain=article["domain"]
                )
            )
            comments_res = self.x_process_comments(article, secname)
            comments_html = comments_res["html"]
            comments_count = comments_res["count"]
            comments_title = (
                "C | {count} | {domain} | {title}".format(
                    count=comments_count,
                    title=article["title"],
                    domain=article["domain"]
                )
            )
            out_articles[article["id"]] = {
                "title": article_title,
                "url": article["url"],
                "date": format_date,
                "comments_title": comments_title,
                "comments_html": comments_html
            }
        return out_articles

    def x_collate_articles(self, articles, dets_map):
        return (
            map(
                (lambda a: dets_map[a["id"]]),
                articles
            )
        )

    def x_build_art_details_all(self, filt_arts):
        dets_map = {}
        for hour, filt_arts_top in filt_arts["top"].items():
            out_top = self.x_build_art_details(filt_arts_top, "top")
            dets_map.update(out_top)
        dets_map_new = self.x_build_art_details(filt_arts["new"], "new")
        dets_map.update(dets_map_new)
        return dets_map

    def x_get_articles(self):
        raw_arts = self.x_fetch_articles()
        filt_arts = self.x_filter_articles(raw_arts)
        dets_map = self.x_build_art_details_all(filt_arts)
        ret = {}
        out_new = self.x_collate_articles(filt_arts["new"], dets_map)
        ret["New"] = out_new
        for hour, filt_arts_top in filt_arts["top"].items():
            out_top = self.x_collate_articles(filt_arts_top, dets_map)
            out_top_name = self.x_build_hour_name("Top", hour)
            ret[out_top_name] = out_top
        return ret

    def x_build_hour_name(self, sectitle, hour):
        return "%s %sh" % (sectitle, hour)

    def x_save_comment_url(self, article):
        tmp = PersistentTemporaryFile('_fa.html')
        tmp.write(article["comments_html"])
        tmp.close()
        self.temp_files.append(tmp)
        url = "".join(["file://", tmp.name])
        return url

    def parse_index(self):
        item_grps = self.x_get_articles()
        feeds = []
        for section, items in item_grps.items():
            articles = []
            for item in items:
                articles.append({
                    "title": item["title"],
                    "url": item["url"],
                    "date": item["date"],
                    "description": item["title"],
                    "content": ""
                })
                articles.append({
                    "title": item["comments_title"],
                    "url": self.x_save_comment_url(item),
                    "date": item["date"],
                    "description": item["comments_title"],
                    "content": ""
                })
            feeds.append((
                section,
                articles
            ))
        return feeds
