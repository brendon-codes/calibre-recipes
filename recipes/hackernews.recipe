#!/usr/bin/env python2
# vim:fileencoding=utf-8

from __future__ import (
    unicode_literals,
    division,
    absolute_import,
    print_function
)
import json
import datetime
from dateutil import tz
from urlparse import urlparse
from urllib2 import HTTPError

from calibre.ebooks.BeautifulSoup import BeautifulSoup as Soup
from calibre.ptempfile import PersistentTemporaryFile
from calibre.web.feeds.news import BasicNewsRecipe


class HackerNewsRecipe(BasicNewsRecipe):
    title = "Hacker News"
    __author__ = "Brendon Crawford"
    oldest_article = 2
    max_articles_per_feed = 100
    auto_cleanup = True
    use_embedded_content = False
    publication_type = "blog"
    timefmt = ""
    temp_files = []
    auto_cleanup_keep = (
        "//article[@id='hn-comments'] | //article[@id='hn-comments']::*"
    )
    extra_css = (
        ".hn-even { background-color: #E3E3E3; } "
        ".hn-odd { background-color: #FFFFFF; } "
    )

    DEBUG_ARTICLE_FETCH = False
    FEED_TITLE = "All"
    ARTICLE_COUNT = 50
    ARTICLE_TYPES = frozenset([
        "story"
    ])
    ARTICLE_DOMAINS_EXCLUDE = [
        # "nytimes.com",
        # "bloomberg.com",
        # "economist.com",
        # "theatlantic.com",
        # "techcrunch.com"
    ]
    ITEM_BASE_URL = "https://hacker-news.firebaseio.com/v0/item/{id}.json"
    COMMENT_COUNTS = [12, 6, 1]
    COMMENT_SCORE = 3
    ARTICLE_DATE_HOURS = 24

    def x_fetch_url(self, url):
        br = self.get_browser()
        try:
            resp = br.open(url)
        except HTTPError, e:
            return None
        data = resp.read()
        return data

    def x_fetch_url_json(self, url):
        content = self.x_fetch_url(url)
        if content is None:
            return None
        obj = json.loads(content)
        return obj

    def x_get_article(self, article_id):
        url = self.ITEM_BASE_URL.format(id=article_id)
        article = self.x_fetch_url_json(url)
        if article is None:
            return None
        return article

    def x_should_skip_art_type(self, article):
        return (article["type"] not in self.ARTICLE_TYPES)

    def x_should_skip_art_domain(self, article):
        if "url" not in article:
            return True
        url = article["url"]
        parts = urlparse(url)
        domain = parts.netloc
        for exc_domain in self.ARTICLE_DOMAINS_EXCLUDE:
            exc_dotdomain = "".join([".", exc_domain])
            if (
                    (exc_domain in domain) or
                    (exc_dotdomain in domain)
            ):
                return True
        return False

    def x_should_skip_article(self, article):
        if self.x_should_skip_art_type(article):
            return True
        if self.x_should_skip_art_domain(article):
            return True
        return False

    def x_build_comments(
            self,
            comments_map,
            comment_ids,
            ret_comments,
            depth=0
    ):
        if (depth >= len(self.COMMENT_COUNTS)):
            return True
        if ((comment_ids is None) or (len(comment_ids) == 0)):
            return True
        comment_count = self.COMMENT_COUNTS[depth]
        #print("COUNT: %d" % comment_count)
        filt_comments = (
            self.x_filter_comments(
                comments_map,
                comment_ids,
                comment_count
            )
        )
        for filt_comment in filt_comments:
            comment_id = filt_comment["id"]
            if comment_id not in comments_map:
                continue
            url = self.ITEM_BASE_URL.format(id=comment_id)
            comment_json = self.x_fetch_url_json(url)
            ret_comments.append({
                "id": comment_id,
                "score": comments_map[comment_id],
                "date": (
                    (
                        datetime.datetime.utcfromtimestamp(
                            int(comment_json["time"])
                        )
                        .replace(tzinfo=tz.tzutc())
                    ) if
                    (
                        (comment_json is not None) and
                        ("time" in comment_json)
                    ) else
                    ""
                ),
                "depth": depth,
                "text": comment_json["text"]
            })
            sub_comment_ids = (
                comment_json["kids"] if
                ("kids" in comment_json) else
                []
            )
            self.x_build_comments(
                comments_map,
                sub_comment_ids,
                ret_comments,
                (depth + 1)
            )
        return True

    def x_fetch_api_comments(self, comments_map, comment_ids):
        comments = []
        self.x_build_comments(comments_map, comment_ids, comments)
        return comments

    def x_get_comments_map(self, article_id):
        base = "https://news.ycombinator.com/item?id={id}"
        url = base.format(id=article_id)
        content = self.x_fetch_url(url)
        soup = Soup(content)
        com_rows = (
            soup.findAll(
                "tr",
                attrs={
                    "class": "athing comtr "
                },
                recursive=True
            )
        )
        comments = {}
        for com_row in com_rows:
            comment = self.x_parse_comment(com_row)
            if comment is None:
                continue
            comments[comment["id"]] = comment["score"]
        return comments

    def x_parse_comment(self, com_row):
        if not com_row.has_key("id"):
            return None
        atogg = (
            com_row.find(
                "a",
                attrs={
                    "class": "togg"
                }
            )
        )
        if atogg is None:
            return None
        if not atogg.has_key("n"):
            return None
        ##
        ## Build output
        ##
        comment = {}
        comment["id"] = int(com_row["id"])
        comment["score"] = int(atogg["n"])
        return comment

    def x_filter_comments(self, comments_map, comment_ids, count):
        out = []
        for pre_comment_id in comment_ids:
            if pre_comment_id not in comments_map:
                continue
            out.append({
                "id": pre_comment_id,
                "score": comments_map[pre_comment_id]
            })
        sorted_coms = (
            sorted(
                out,
                key=lambda c: c["score"],
                reverse=True
            )
        )
        filt_comments = (
            filter(
                (lambda c: c["score"] >= self.COMMENT_SCORE),
                sorted_coms
            )
        )
        comments = filt_comments[:count]
        return comments

    def x_process_comments(self, article):
        comments_map = self.x_get_comments_map(article["id"])
        comments = (
            self.x_fetch_api_comments(comments_map, article["comment_ids"])
        )
        html = self.x_format_comments(article, comments)
        return html

    def x_format_comments(self, article, comments):
        page_title = "Comments: {title}".format(title=article["title"])
        out = "<!doctype html>\n"
        out += "<html lang='en'>\n"
        out += "<head>\n"
        out += "<meta charset='utf-8' />\n"
        out += (
            "<title>{title}</title>\n".format(title=page_title)
        )
        out += "</head>\n"
        out += "<body>\n"
        out += "<article id='hn-comments'>\n"
        out += (
            "<h1>{title}</h1>\n".format(title=page_title)
        )
        out += "<div>\n\n"
        for comment in comments:
            real_depth = comment["depth"] + 1
            stars = " ".join(map(lambda x: "+", xrange(real_depth)))
            comment_date = (
                comment["date"]
                .astimezone(tz.tzlocal())
                .strftime('%Y-%m-%d %I:%M:%S%p')
            )
            out += "<div>\n"
            out += (
                "<h3>{stars} | {score} | {date}</h3>\n".format(
                    stars=stars,
                    score=comment["score"],
                    date=comment_date
                )
            )
            out += "<div>\n"
            out += comment["text"]
            out += "\n"
            out += "</div>\n"
            out += "</div>\n\n"
        out += "</div>\n"
        out += "</article>\n"
        out += "</body>\n"
        out += "</html>\n"
        return out

    def x_fetch_articles(self):
        url = "https://hacker-news.firebaseio.com/v0/topstories.json"
        new_article_ids = self.x_fetch_url_json(url)
        if new_article_ids is None:
            return []
        i = 0
        out_articles = []
        now = datetime.datetime.utcnow().replace(tzinfo=tz.tzutc())
        for article_id in new_article_ids:
            self.report_progress(0, "Fetching Article: %s" % article_id)
            article = self.x_get_article(article_id)
            if article is None:
                continue
            if self.x_should_skip_article(article):
                continue
            article_date = (
                datetime.datetime.utcfromtimestamp(
                    int(article["time"])
                )
                .replace(tzinfo=tz.tzutc())
            )
            offset = now - article_date
            cutoff = (self.ARTICLE_DATE_HOURS * 3600)
            offsecs = int(offset.total_seconds())
            if offsecs > cutoff:
                continue
            out_articles.append({
                "id": article["id"],
                "title": article["title"],
                "url": article["url"],
                "score": article["score"],
                "date": article_date,
                "comment_ids": (
                    article["kids"] if
                    ("kids" in article) else
                    []
                )
            })
            ##
            ## Debug
            ##
            if self.DEBUG_ARTICLE_FETCH and (i > 5):
                break
            i += 1
        final_articles = (
            sorted(
                out_articles,
                key=lambda a: a["score"],
                reverse=True
            )
        )
        limit_articles = final_articles[:self.ARTICLE_COUNT]
        return limit_articles

    def x_get_articles(self):
        articles = self.x_fetch_articles()
        out_articles = []
        for article in articles:
            self.report_progress(0, "Processing Article: %s" % article["id"])
            format_date = article["date"].strftime('%Y-%m-%d %H:%M:%S')
            article_title = (
                "[{score}] {title}".format(
                    score=article["score"],
                    title=article["title"]
                )
            )
            comments_title = "[Comments] {title}".format(title=article["title"])
            comments_html = self.x_process_comments(article)
            out_articles.append({
                "title": article_title,
                "url": article["url"],
                "date": format_date,
                "comments_title": comments_title,
                "comments_html": comments_html
            })
        #print(out_articles)
        return out_articles

    def x_save_comment_url(self, article):
        tmp = PersistentTemporaryFile('_fa.html')
        tmp.write(article["comments_html"])
        tmp.close()
        self.temp_files.append(tmp)
        url = "".join(["file://", tmp.name])
        return url

    def parse_index(self):
        items = self.x_get_articles()
        articles = []
        for item in items:
            articles.append({
                "title": item["title"],
                "url": item["url"],
                "date": item["date"],
                "description": item["title"],
                "content": ""
            })
            articles.append({
                "title": item["comments_title"],
                "url": self.x_save_comment_url(item),
                "date": item["date"],
                "description": item["comments_title"],
                "content": ""
            })
        feeds = []
        feeds.append((
            self.FEED_TITLE,
            articles
        ))
        return feeds
